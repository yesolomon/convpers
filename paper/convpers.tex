\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools}
\usepackage[alphabetic,backrefs,lite,nobysame]{amsrefs}
\usepackage{algorithm2e}
\usepackage{tikz-cd}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\Edit}[1]{\textcolor{gray}{(#1)}}
\newcommand{\lp}{\operatorname{lp}}
\newcommand{\sh}{\operatorname{sh}}
\newcommand\pder[2][]{\ensuremath{\frac{\partial#1}{\partial#2}}} 
\newcommand{\R}{\mathbb{R}}
\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\PD}{PD}
\DeclareMathOperator{\CONV}{conv}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{mr}[theorem]{Main Result}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\theoremstyle{definition}
\newtheorem{obs}[theorem]{Observation}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{convention}[theorem]{Convention}
\numberwithin{figure}{section}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{algthm}[theorem]{Algorithm}

\graphicspath{{./img/}}

\begin{document}
	
	
	\title{Convolutional Persistence for Cubical Complexes
		\thanks{The  authors were partially supported by the Air Force Office of Scientific Research under the  grant ``Geometry and Topology for Data Analysis and Fusion",  AFOSR FA9550-18-1-0266.}
	}
	
	\author{
		Elchanan Solomon\\
		Department of Mathematics, \\
		Duke University\\
		Durham, USA \\
		yitzchak.solomon@duke.edu
		\and
		Paul Bendich\\
		Department of Mathematics, Duke University\\
		Geometric Data Analytics\\
		Durham, USA \\
		paul.bendich@duke.edu
	}
	
	\maketitle
	
	\begin{abstract}
		todo.
	\end{abstract}
	
\section{Overview}
Persistent homology is a method of assigning multiscale topological descriptors to parametric families of shapes. In \emph{functional} persistence, the object of study is a real-valued function $f:X \to \mathbb{R}$ defined over a space $X$, and the parametric family of shapes are the sublevel-sets $X_{\alpha} = \{x \in X : f(x) \leq \alpha\}$. It is similarly possible to consider superlevel-sets, which is equivalent to negating the \emph{filter function} $f$. One crucial feature of this construction is that $X_{\alpha}$ is a subset of $X_{\beta}$ for $\alpha \leq \beta$, so that the sublevel-sets are naturally nested. The output of persistent homology is a collection of intervals (equivalently, a collection of points), called a \emph{barcode} (or \emph{persistence diagram}, using the point representation). The space of barcodes is not a vector space, even approximately {\bf (cite!)}, but there do exist multiple \emph{vectorizations} that transform barcodes into vectors suitable for machine learning and data analysis.\\

A very general setting for functional persistence is that of \emph{cubical complexes}, i.e. shapes obtained by gluing together cubical regions like pixels or voxels. For example, if $X$ is a 2D rectangular grid, a function $f:X \to [0,1]$ can be viewed as a greyscale image. Persistent homology can then be understood as a feature extraction method for such images, either for supervised or unsupervised learning. Example applications include {\bf cite: removing image noise (Chung), parameter estimation for PDEs (Calcina, Adams), segmentation (Chen), flow estimation (Suzuki), tumor analysis (Crawford), cell immune micronenvironment (Carriere), and materials science (Hiraoka)}.\\

It should be understood that purely topological methods do not provide state-of-the-art predictive accuracy on most machine learning tasks, and are not to be considered as \emph{alternatives} to more general methods like neural nets and kernel methods. Rather, topological methods provide \emph{principled} and \emph{intepretable} features that are different from those extracted by other methods, and can help improve the performance and utility of the entire pipeline.

To that end, it is important to understand the properties of functional persistence and their role in machine learning. Here we consider some of the most salient features:
\begin{itemize}
	\item (Computational complexity) For a cubical complex $K$ with $N$ cubes, computing persistence is $O(N^\omega)$, where $\omega$ is the matrix multiplication constant. This means that such persistence calculations scale poorly in the resolution of data, especially high-dimensional data, where doubling the resolution in $\mathbb{R}^d$ results in a $2^{d}$-fold increase in the number of simplices.
	\item (Stability) Persistent homology is stable to small perturbations of the input data, in that the distance between the barcodes for two functions $f$ and $g$ on a common space $X$ is bounded by $\|f-g\|_{\infty}$. However, persistence is not stable to outliers, so images that look similar outside of a small fraction of pixels can produce wildly different barcodes.
	\item (Flexibility) There is a single persistence diagram to be associated with each pair $(X,f)$ of space $X$ and real-valued function $f$. This lack of additional parameters makes applying persistence straightforward, but can also be limiting in contexts where featurizations should be data-dependent. 
	\item (Invertibility) There exist many distinct space-function pairs $(X,f)$ producing identical barcodes. Thus, persistence is not invertible as feature map, and this loss of information may hinder the capacity of persistence-based methods to identify patterns or distinguish distinct images.
\end{itemize}

Our goal in this paper is to introduce a modification to persistence of certain cubical complexes that is (1) faster to compute, (2) more stable to outliers, (3) allows for data-driven tuning, and (4) is provably invertible. Essentially, this technique consists of passing multiple convolutional filters over an image and computing the persistence of the resulting collection of low-resolution outputs. Since this pipeline consists principally of combining convolutions with persistence, we call it \emph{convolutional persistence}.\\

The remainder of the paper is organized as follows. Section II provides a thorough, non-technical survey on persistent homology. Section III defines \emph{convolutional persistence} and reviews prior related work. Section IV contains the major theoretical results of the paper, with the highlight being {\bf Theorem ?} proving injectivity for convolutional persistence. Section V compares ordinary and convolutional persistence on a host of datasets, showing the capacity of convolutional persistence to produce features well-suited for image classification. Finally, Section VI discusses outstanding questions and generalizations of this work.

\section{Background}
\label{sec:background}
The content of this paper assumes familiarity with the concepts and tools of persistent homology. Interested readers can consult the articles of Carlsson \cite{carlsson2009topology} and Ghrist \cite{ghrist2008barcodes} and the textbooks of 
Edelsbrunner and Harer \cite{edelsbrunner2010computational} and Oudot \cite{oudot2015persistence}. We include the following primer for readers interested in a high-level, non-technical summary. Since this work is concerned with cubical complexes, we focus on persistent homology in that context. 

\subsection{Persistent Homology}

Persistent homology records the way topology evolves in a parametrized sequence of spaces. In the case of functional persistence, we consider an ambient space $X$ filtered by a real-valued function $f:X \to \mathbb{R}$. When $X$ is a cubical complex, made by gluing together cubes of varying dimension (vertices, edges, squares, cubes, etc.), this construction is particularly simple. To every cube $\sigma$ we associate a real value $f(\sigma)$ that encodes the parameter value at which it appears in the filtration of $X$. The only restriction on the function $f$ is the following consistency condition: if $\sigma$ is a sub-cube of a higher-dimensional cube $\tau$ (i.e. an edge which sits at the boundary of a square), we must have $f(\sigma) \leq f(\tau)$, ensuring that cubes do not appear before any of their faces.\\


Given a filtered cubical complex, as the sequence of sublevel-sets evolves, the addition of certain edges or higher-dimensional cubes alters the \emph{topological type} of the space. A precise way of quantifying topology is \emph{homology}, which measures the number of connected components (zero-dimensional homology), cycles (one-dimensional homology), or voids (higher-dimensional homology) in a space. Thus, homology can change when two connected components merge or a new cycle is formed. Cubes responsible for such topological changes are called \emph{critical}. Persistent homology records the parameter values at which critical cubes appear, notes the dimension in which the homology changes, and pairs critical values by matching the critical value at which a new homological feature appears to the critical value at which it disappears. This information is then organized into a structure called a \emph{barcode}, which is simply a collection of intervals. Figure \ref{fig:cubpers} shows the computation of the zero- and one-dimensional barcodes for a simple cubical complex.\\

A barcode can also be encoded as a collection of points, simply by mapping each interval $[a,b]$ to the point $(a,b) \in (\mathbb{R}\cup \{-\infty\} \cup \{\infty\})^2$. This collection of points is called a \emph{persistence diagram}, and besides for certain subtleties, such as dealing with open or closed endpoints and  keeping track of points with multiplicity, it contains the exact same information as the original barcode. However, for certain analyses and vectorizations of persistence features, persistence diagrams are more useful than barcodes, as will be shown below.\\

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{cubpers.eps}
	\caption{Top-left: A cubical complex with filtration values attached to vertices, edges, and squares. Top-right through bottom-left: sublevel-sets associated with different threshold values. Bottom-right: barcodes in dimensions zero and one. The zero-dimensional homology $H_0$ barcode contains four bars, since at $\alpha=0$ there are four connected components. Two bars die at $\alpha=1$, since at that threshold value there are only two connected components. Finally, $\alpha=2$ sees the merger of these connected component, so another bar dies at $\alpha=2$ and the last persists to infinity. In one-dimensional homology $H_1$, three bars are born at $\alpha = 2$, when three loops appear in the sublevel-set, and one of these bars dies at $\alpha=3$, when that loop is killed off by the introduction of a square.}
	\label{fig:cubpers}
\end{figure}

When working with cubical complexes, it is possible to limit the maximal dimension of cubes allowed in the construction. Given an arbitrary cubical complex $K$, we write $K^m$ to denote the subcomplex consisting of all cubes of dimension at most $m$; this is called the \emph{$m$-skeleton of $K$}. Note that $K$ and $K^m$ have the same homology in dimensions less than $m$, but may differ in dimension $m$, and $K^m$ has no homology in dimension greater than $m$.

\subsection{Image Cubical Complexes}
Given a $d$-dimensional grayscale image, there are two ways of turning this data into a cubical complex. One is to view the voxels as being vertices, and higher-dimensional cubes as coming from voxels adjacencies, so that pairs of adjacent pixels form an edge and squares come from four voxels in a square formation, etc. There is a canonical way of extending the function $f$ from the voxels (vertices) to the entire complex: given a cube $\tau$, define $f(\tau)$ to be $\max_{\sigma} f(\sigma)$, where the max is taken over all vertices $\sigma < \tau$. Thus, a square appears precisely once all its constituent vertices appear; this is called the \emph{lower-*} filtration.\\

Alternatively, one can also view the voxels as being $d$-dimensional cubes, and have the lower-dimensional cubes be the faces of these voxels. As before, there is a canonical way of extending the function value from the voxels (top-dimensional cubes) down to entire complex: for a cube $\sigma$, define $f(\sigma)$ to be $\min_{\tau} f(\tau)$, where the min is taken over all voxels $\tau$ that contain $\sigma$. Thus, a cube appears precisely when at least one of the voxels in which it participates does;  this is called the \emph{upper-*} filtration.\\ 

Consult figure \ref{fig:image_to_complex} for an illustration of these two images complexes. Generally, these complexes will differ, and the resulting persistence barcodes will be different. However, there exists a formula for reading the barcodes for one construction from the barcodes of the other, see \cite{bleile2021persistent}.
In the experimental section of this paper, we will adopt the latter construction, taking our voxels to be the top-dimensional cubes of the cubical complex.


\begin{figure}
	\centering
	\includegraphics[scale=2]{image_to_complex.eps}
	\caption{A $2 \times 2$ image can be turned into a complex in one of two ways. Left: A complex with four top-dimensional cubes, with function values, indicated using color, extended to vertices and edges via the upper-* rule. Right: A complex with four vertices, with function values extended to the edges and interior square via the lower-* rule.}
	\label{fig:image_to_complex}
\end{figure}
\subsection{Comparing Persistence Diagrams}

As multi-sets of points in the plane, persistence diagrams are not vectors. However, there exist multiple metrics for comparing persistence diagrams. The most common approach is to view persistence diagrams as discrete distributions on the plane $\mathbb{R}^2$, and use techniques from optimal transport theory, such as $p$-Wasserstein metrics $W_p$, to compare them. This analogy is complicated by the fact that persistence diagrams do not all have the same number of points, and that points in a persistence diagram near the diagonal line $y=x$ correspond to transient homological features, dying shortly after they are born, which ought not to play an important role in dictating similarity of diagrams. These problems are ameliorated by modifying the optimal transport protocol to allow paring points in one diagram either with points in the other diagram or with the diagonal line $y=x$, the latter incurring a cost proportional to the distance of the given point from the diagonal. This \emph{diagonal paring} allows us to define an optimal transport distance between any pair of diagrams, regardless of how many points they have. In practice, the $p$-Wasserstein metrics in use are either $p=1,p=2$, or $p=\infty$, the latter of which is called the \emph{Bottleneck distance} in persistence theory, and is written $d_B$. See Figure \ref{fig:wasserstein} for a visualization of a matching between persistence diagrams.  

\begin{figure}
	\centering
	\includegraphics[scale=1]{wasserstein.eps}
	\caption{Two persistence diagram $D_1$ and $D_2$, one in black and the other in blue. An optimal matching between these diagrams is shown, containing both diagonal and non-diagonal pairings. The resulting $p$-Wasserstein distance is then $W_{p}(D_1,D_2) = \sqrt[p]{d_1^p + d_2^p + d_3^p + d_4^p + d_5^p}$.}
	\label{fig:wasserstein}
\end{figure}


\subsection{Computational Complexity}
Persistence calculations are $O(N^{\omega})$, where $N$ is the number of cubes in the filtration and $\omega$ is the matrix multiplication constant \cite{milosavljevic2011zigzag}. Though this bound is in practice quite pessimistic, it is accurate in reflecting the poor scaling of persistence calculations in the resolution of an image \cite{otter2017roadmap}.\\

The bottleneck distance between persistence diagrams can be computed exactly using the well-known Hungarian algorithm, whose complexity is $O(n^3)$ for pairs of diagrams with at most $n$ points. The number of points in the persistence diagram of filtration is always bounded by the total number $N$ of simplices in the filtration, and may be much smaller. If one is interested in \emph{approximating} the bottleneck, or more generally, $p$-Wasserstein distances between persistence diagrams, there are significantly faster methods than the Hungarian algorithm. One such method is based on \emph{entropic approximation} techniques from optimal transport \cite{lacombe2018large}, and provided that the norms of the points in the diagrams are uniformly bounded by some constant $C$, a transport plan within $\epsilon$ of the optimal matching can be produced in $O(\frac{n \log(n) C}{\epsilon^2})$ iterations of the Sinkhorn algorithm, which is itself linear in $n$. 

%This algorithm is implemented in the persistent homology library GUDHI that we use in our experiments, and hence the computational complexity of our framework as a whole is dominated by the calculation of the diagrams themselves, and not the $p$-Wasserstein distances between them.

\subsection{Properties of Persistent Homology}
\label{subsec:persprop}
Persistence theory guarantees that a small modification to the filtration of a space produces only small changes in its persistence diagram. To be precise, if $f,g:X \to \mathbb{R}$ are two filtrations on a fixed space, the Bottleneck distance between their persistence diagrams is bounded by $\|f-g\|_{\infty}$  \cite{cohen2007stability}. This implies that a small error in the filter function produces only small distortion in the resulting persistence diagram. However, persistent homology is not at all stable to \emph{outliers}, i.e. a small subset of the data having large error \cite{buchet2014topological}.\\ 

Another important feature of persistent homology is that the pipeline mapping a function $f$ on a cubical complex $K$, thought of as a vector in $\mathbb{R}^K$, to its associated persistence diagram is  \emph{almost everywhere differentiable}  \cite{gameiro2016continuation,poulenard2018topological}. This is because the coordinates of the points in the diagram correspond to the values of $f$ on critical cubes, and this correspondence is generically locally stable. However, large perturbations of $f$ will change the location of critical cubes, as well as their pairings, and non-differentiable nonlinearities can be found in those degenerate cases when the critical cube data is not unique and locally unstable. This almost everywhere differentiability of persistent homology allows us to use gradient descent methods for topological optimization  \cite{carriere2021optimizing}.\\ 


It is also important to note that persistent homology is not an injective invariant of metric spaces, meaning that different spaces can have the same persistence diagrams \cite{curry2018fiber,leygonie2021fiber,leygonie2021algorithmic}. Thus, some information is lost in the process of converting a metric space to a persistence diagram.

\section{Convolutional Persistence}

Our goal is to introduce a modification to functional persistence of cubical complexes based on the technique of \emph{convolutions} in image processing. To that extent, we specialize to cubical complexes coming from integer grids. To be precise, let $P \subset \mathbb{Z}^d$ be a rectangle inside of the integer lattice, and let $f: P \to \mathbb{R}$ be a function defined on $P$. $P$ can be viewed as the vertex set of an $m$-dimensional cubical complex $K^{m}_{P}$ in which higher-dimensional simplices are given by grid adjacencies, and $f$ can be extended to this complex via the lower-* rule, in which $f(\sigma) = \max_{p \in \sigma}f(p)$, as in {\bf cite relevant background section}.\\

\begin{definition}
Let $B \subset \mathbb{Z}^d$ be another rectangle, smaller than $P$, and let $g: B \to \mathbb{R}$ be a function on this rectangle; the pair $(B,g)$ acts as a \emph{convolutional filter}. Fix a vector $k = (k_1, \cdots, k_d)$ with $k_i \in \mathbb{N}_{>0}$, corresponding to the \emph{stride} of the convolution. For $v \in \mathbb{Z}^d$ with $B + (v \odot k) \subseteq P$,\footnote{The symbol $\odot$ refers to the Hadamard product, which performs componentwise multiplication between vectors.} define:
\[(g \ast f)(v) = \sum_{p \in B} g(p)f(p+v \odot k).\]
Let $R \subset \mathbb{Z}^d$ be the collection of values $r$ such that $B + (r \odot k) \subseteq P$, which is necessarily also a rectangle. The pair $(R, g \ast f)$ is the output of our convolution.
\end{definition}

$R$ may be much smaller than the support of $f$, since the various translates of $B$ covering this support are not required to overlap much, if at all. In the degenerate setting where $P$ and $B$ are the same shape, $R$ will consist of a single vertex. For the purposes of computing persistence, we will think of $R$, like $P$, as being the vertex set of a cubical complex $K_{R}^{m}$, and we extend functions on $R$ to the entirety of the cubical complex using the lower-* rule.\\

We propose that the collection of persistence diagrams of the form $PH(K_{R}^{m},g_{i} \ast f)$, for some set of filter functions $\mathcal{G} = \{g_i\}$, is of greater general utility than $PH(K_{P}^{m},f)$. The computational advantages are immediate: when the stride $k$ is large, so that $R$ is much smaller than $P$, we have replaced a single, very expensive calculation with multiple, significantly faster calculations that can be performed in parallel. Moreover, we claim that this approach, which we deem \emph{convolutional persistence}, is more robust and flexible than ordinary persistence, and has superior inverse properties.\\

%It is also important to note that the \emph{translation equivariance} of convolutions and the \emph{transational invariance} of persistent homology work well together.\\


%\emph{Stability and robustness} are not hard to justify. If $g$ is a filter function with $\|g\|_{1} \leq 1$, then $\|g \ast f_1 - g \ast f_2 \|_{\infty} \leq \|f_1 - f_2\|_{\infty}$ by Young's convolution inequality, providing stability. A classical example is the \emph{box-smoothing} filter $g(p) = \frac{1}{|B|}$. Such smoothing filters also do a good job of diminishing the impact of outliers Invertibility is the focus of {\bf cite section here}.\\


%Now, what we have described above is persistence with a single convolutional layer. One can extend this construction by convolving the collection $\{(R, f \ast g) \mid g \in \mathcal{G}\}$ using a new rectangle $B'$ and collection $\mathcal{H}$ of filter functions $h: B' \to \mathbb{R}$, and \emph{then} compute persistence. This pipeline remains injective, because the collection $\{(R, g \ast f) \mid g: B \to \mathbb{R}\}$ determines the function $f$ (this is trivial, consider characteristic functions of pixels in $B$), so the two-layer convolutional persistence is a composition of two injective operations, where we are now taking the sets $\mathcal{G}$ and $\mathcal{H}$ to consist of \emph{all possible filters}. We might also consider introducing a non-linear activation between the convolutional layers, following more closely the structure of a CNN.\\
%
%An alternative convolutional pipeline is to do a convolutional with a small stride, so that the resulting image has the same resolution as the original, and then apply a pooling later. This mirrors more closely the structure of a modern CNN. Note that this pipeline also enjoys an inverse result, provided that the corresponding embedding is injective on the input cubical complex.\\


%In practice, of course, we are not interested in considering infinitely many filter functions. Indeed, learning tasks require invariants to \emph{retain} features that are important to classification or regression, and \emph{forget} features that are not. Thus, we will want to pick $\mathcal{G}$ appropriately. 

The flexibility of convolutional persistence comes from allowing the collection of filters $\mathcal{G}$ to be curated for the task at hand. Indeed, there are many ways for this choice to be made:
\begin{enumerate}
	\item Take $\mathcal{G}$ to consist of a collection of popular filters in image processing, like blurring, sharpening, and boundary detection.
	%\item Take $\mathcal{G}$ to consist of Klein filters, as identified by Carlsson et al.
	\item Take $\mathcal{G}$ to consist of eigenfilters identified via PCA on the set of patches of images in the training set.
	\item Picking filters \emph{at random}.
	\item Incorporate convolutional persistence in a deep learning pipeline and learn $\mathcal{G}$ to minimize a chosen loss function.
\end{enumerate}

In the {\bf cite experimental section}, we compare some of these different choices on a range of data sets and learning tasks.

\subsection{Prior Work}
Convolutional persistence is a type of \emph{topological transform}, in which a parametrized family of topological invariants is associated to a fixed input. The first topological invariants studied were the \emph{persistent homology transform} (PHT) and \emph{Euler characteristic transform} (ECT) \cite{turner2014persistent}. In the same paper in which the PHT and ECT were defined, inverse theorems for provided; these theorems were generalized and extended by later work \cite{curry2018many,ghrist2018persistent}. We will later show that convolutional persistence is, generically, a special case of the PHT, allowing us to take advantage of the inverse theory developed for that invariant. Invertible topological transforms have also been developed for weighted simplicial complexes \cite{jiang2020weighted}, metric graphs \cite{oudot2017barcode}, metric spaces \cite{solomon2021geometry}, and metric measure spaces \cite{maria2019intrinsic}. Consult \cite{oudot2020inverse} for a more thorough survey of inverse problem in applied topology.\\

Prior research has also studied the interaction of persistent homology and image convolutions. In \cite{solomon2021fast}, the authors use convolutions to stabilize and speed up topological optimization; the approach taken there can be viewed as a special case of convolutional persistence, in which the filter set $\mathcal{G}$ consists of many random approximations of the box-smoothing filter. Unlike in this work, the goal of \cite{solomon2021fast} is to produce many downsampled images with persistence similar to the original, permitting the computation of robust topological gradients. Another paper that considers both persistence and convolutions is \cite{kim2020pllay}, where the authors develop a topological layer for deep learning models; one such model they consider is a convolutional neural network, and the authors design the network to compute topological features both before and after computing convolutions. However, \cite{kim2020pllay} do not study the interaction of convolutions and persistence in any generality. \\

\section{Theoretical Results}

\begin{prop}[Stability]
	Let $P,(B,g),R,m$ be as in the definition of convolutional persistence, and let $f_1,f_2: P \to \mathbb{R}$ be two functions on $P$. Then for any $p\geq 1$ we have:
	\begin{align*}
	W_{p}(\operatorname{Diag}(f_1 \ast g),\operatorname{Diag}(f_2 \ast g)) & \leq\\ \left(\sum_{i=1}^{m} 2^{m-i} {m \choose i}\right)\|g\|_{1} \|f_1 - f_2\|_{p} & \leq\\ |B| \left(\sum_{i=1}^{m} 2^{m-i} {m \choose i}\right)\|g\|_{\infty} \|f_1 - f_2\|_{p}.
	\end{align*}
	
	
\end{prop}
\begin{proof}
	Young's convolutional inequality states that for $p,q,r \geq 1$ with $\frac{1}{p} + \frac{1}{q} = \frac{1}{r} + 1$,
	\[\|f \ast g\|_{r} \leq  \|g\|_{q} \|f\|_{p}.\]
	
	Setting $r=p$ forces $q=1$, and setting $f = f_1 - f_2$, we obtain the bound $\|f_1 \ast g - f_2 \ast g\|_{p} \leq \|g\|_{1}\|f_1 - f_2\|_{p}$. Since $g$ is supported on a set of $|B|$ elements, we have $\|g\|_{1} \leq |B|\|g\|_{\infty}$, so that $\|f_1 \ast g - f_2 \ast g\|_{p} \leq |B| \|g\|_{\infty}\|f_1 - f_2\|_{p}$. The bound then follows from Theorem 5.1 in \cite{skraba2020wasserstein}, which bounds the $p$-Wasserstein distance between the persistence diagrams associated to two images $f_1,f_2$, in terms of the $p$-norm of their difference.
\end{proof}

We now prove the major theoretical result of the paper, for which we require some preliminary definitions and results.

\begin{definition}
Let $P,(B,g),R,m$ be as in the definition of convolutional persistence. For a fixed function $f:P \to \mathbb{R}$ and stride vector $k$, we can obtain a mapping $\iota_{f}$ of the rectangle $R$ into $\mathbb{R}^{|B|}$ by sending every point $r \in R$ to the vector $\{f(b + k \odot r) \mid b \in B\}$. This is technically a set, rather than a vector, but it becomes a vector after fixing an order on the elements of $B$. Such a mapping can be extended linearly to the entire cubical complex $K_{R}$ built on top of $R$. Let us call the function $f$ \emph{m-good} if $\iota_{f}$ is injective on the $m$-skeleton $K_{R}^m$, so that $\iota_{f}(K_{R}^m)$ has the structure of a simplicial complex isomorphic to $K_{R}^m$.
\end{definition}

\begin{lemma}
Suppose that $\kappa = \Pi_{i}k_i > 2m$. Then a generic function $f$ is $m$-good.
\end{lemma}
\begin{proof}
	Let $B^{*} \subseteq B$ consist of those elements in the top-left $k_1 \times k_2 \times \cdots k_d$ corner of $B$. The various translates $B^{*} + (k \odot r)$ are all disjoint subsets of $P$. We can thus view the vertices $\{f(b + k \odot r) \mid b \in B\}$ of $\iota_{f}(K_{R}^m)$ as having at least $\kappa$ degrees of freedom, and this can be made more precise by projecting the embedding on to the coordinates corresponding to $B^*$. We now want to show that if we choose a collection $S$ of points in $\mathbb{R}^{M}$ for $M > 2m$, and build the complete $m$-dimensional complex on top of these points, we will generically never have nontrivial intersections of disjoint simplices. To see why this is the case, let $\sigma$ and $\tau$ be simplices corresponding to disjoint subsets $S_{\sigma}$ and $S_{\tau}$ of $S$. If $\sigma$ and $\tau$ intersect, then their union is contained in an affine subset of $\mathbb{R}^M$ of dimension $(\dim \sigma + \dim \tau)$. Now, whenever $M > \dim \sigma + \dim \tau$, a collection of at least $(\dim \sigma + \dim \tau + 2)$ points generically does not lie on an affine subset of dimension $(\dim \sigma + \dim \tau)$. Since $M > 2m \geq \dim \sigma + \dim \tau$, and $|S_{\sigma} \cup S_{\tau}| = |S_{\sigma}| + |S_{\tau}| = (\dim \sigma + 1) + (\dim \tau + 1) = \dim \sigma + \dim \tau + 2$, we see that the intersection of $\sigma$ and $\tau$ is not generic.
\end{proof}

\begin{definition}[Convolutional Persistence Transform]
Let $f: P \to \mathbb{R}$, and fix a filter box $B$, stride vector $k$, and skeleton parameter $m$. The \emph{Convolutional Persistence Transform}  is a mapping $CPT: \mathbb{S}^{|B|-1} \to \mathbf{Diagrams}$ that sends a normalized function $g: B \to \mathbb{R}$ to the persistence diagram of $f \ast g$ on $K_{R}^{m}$. One can similarly define an \emph{Euler characteristic curve} version of this invariant, the \emph{Convolutional Euler Characteristic Transform} (CECT), by computing Euler curves instead of persistence diagrams. 
\end{definition}

\begin{theorem}[Generic Injectivity of CPT]
Fix convolutional persistence parameters $P,B,k,m$. The $CPT$ and $CECT$ are injective on the set of $m$-good functions. 
\end{theorem}
\begin{proof}
By hypothesis, if $f$ is $m$-good, $\iota_{f}$ is injective on the $m$-skeleton $K_{R}^m$, so that $\iota_{f}(K_{R}^m)$ has the structure of a simplicial complex isomorphic to $K_{R}^m$. Then for a function $g: B \to \mathbb{R}$, viewed also as a vector $\vec{g} \in \mathbb{R}^{|B|}$, and threshold value $t$, the sets $\{\sigma \in K_{R}^{m} \mid g(\sigma) \leq t\}$ and $\{\sigma \in \iota_{f}(K_{R}^m) \mid \sigma \cdot \vec{g} \leq t \}$ are homeomorphic. See Figure \ref{fig:embedding} for a visual schematic. This means that the persistence diagram $PH(K_{R}^m,g \ast f)$ is identical to the persistence diagram $PH(\iota_{f}(K_{R}^m),\langle g, \cdot \rangle)$. If $g$ is varied over the unit sphere in $\mathbb{R}^{|B|}$, the set of diagrams $PH(\iota_{f}(K_{R}^m),\langle g, \cdot \rangle)$ is known as the \emph{Persistent Homology Transform} (PHT) which is known to be injective, see \cite{turner2014persistent,ghrist2018persistent,curry2018many}. Thus, the collection of persistence diagrams of the form $\{PH(K_{R}^m, g \ast f) \mid g : B \to \mathbb{R}\}$ is injective. Injectivity of the Euler characteristic curve version of the PHT then provides injectivity for the CECT.
\end{proof}

	
\begin{figure}
	\includegraphics{embedding}
	\caption{The function $f$ is defined on the $4 \times 4$ grid on the top left. The box $B$ is $3 \times 3$, and using a $(1,1)$-stride there are four ways of laying $B$ over the domain of $f$, so that $R$ is a $2 \times 2$ grid. We can map the vertices of $R$ into $\mathbb{R}^9$ by associating each vertex of $R$ with its corresponding translate of $B$, and then taking as coordinates the values of $f$ in that translate. This extends linearly to a map $\iota_{f}$ from the complex $K_{R}^2$ in $\mathbb{R}^9$, which here is shown to be an embedding.}
	\label{fig:embedding}
\end{figure}

\begin{remark}
	What we have just shown is that having a large stride vector, in addition to providing computational speedups by lowering the resolution of the resultant grid, also provides generic injectivity for persistence of higher-dimensional data complexes.
\end{remark}

The next injectivity result shows that distinguish two functions $f_1$ and $f_2$, it suffices to use direction vectors in the span of $\iota_{f_1}(K_R^m) \cup \iota_{f_2}(K_R^m)$, which can be interpreted in terms of a basis of patches for generating the images $f_1$ and $f_2$.

\newpage


%Now, what we have described above is persistence with a single convolutional layer. One can extend this construction by convolving the collection $\{(R, f \ast g) \mid g \in \mathcal{G}\}$ using a new rectangle $B'$ and collection $\mathcal{H}$ of filter functions $h: B' \to \mathbb{R}$, and \emph{then} compute persistence. This pipeline remains injective, because the collection $\{(R, g \ast f) \mid g: B \to \mathbb{R}\}$ determines the function $f$ (this is trivial, consider characteristic functions of pixels in $B$), so the two-layer convolutional persistence is a composition of two injective operations, where we are now taking the sets $\mathcal{G}$ and $\mathcal{H}$ to consist of \emph{all possible filters}. We might also consider introducing a non-linear activation between the convolutional layers, following more closely the structure of a CNN.\\

%An alternative convolutional pipeline is to do a convolutional with a small stride, so that the resulting image has the same resolution as the original, and then apply a pooling later. This mirrors more closely the structure of a modern CNN. Note that this pipeline also enjoys an inverse result, provided that the corresponding embedding is injective on the input cubical complex.\\

%For the third point above, note that the inverse results for the PHT do not require all sphere directions when the shape in question sits inside a low-dimensional space of Euclidean space. Thus, it makes sense to ignore filters orthogonal to the space of image patches in question.\\

%and this introduces an important tradeoff in convolutional optimization. When the convolutional rectangle $R$ and the stride $k$ are large, the decrease is resolution is quite substantial, yet the number of free parameters to be learned is large. This makes optimization difficult, but also allows for more sophisticated filters. Conversely, when the rectangle and stride are small, there is a more minor decrease in resolution, but a simpler optimization task. As for learning more complicated filters, this is still possible when there are multiple convolutional layers.\\ 
 
  


{\bf Experimental Pipeline:}
\begin{enumerate}
	\item In the first phase of the pipeline, we consider the digits dataset, which is small enough that ordinary persistence calculations are very fast. Thus, the role of this phase is to understand how convolutions improve the utility of persistence features. The first step of this phase is to see how standard ML models perform on digit classification based on cubical persistence features. We can then enrich this featurization using different choices of fixed filters $\mathcal{G}$, as outlined above, and see the impact this has on performance. The fundamental question here is whether or not reasonably good performance can be achieved through convolutional persistence. 
	\item The second phase of the pipeline is to try and learn the filters $\mathcal{G}$ using a deep learning architecture. This will require more work than choosing fixed filters as above.
	\item The third phase is to extend the experiments above to larger data sets, like CIFAR, where the downsampling aspect of the convolutions will also be of importance.
\end{enumerate}


\bibliography{convpersbib}
%
%
%$K$ be a pure $d$-dimensional simplicial complex, meaning that every simplex in $K$ is the face of some $d$-simplex. Let $f: K \to \mathbb{R}$ be a function on the top-dimensional simplices of $K$ extended to all of $K$ via the upper-* filtration: $f(\sigma) = \min \{f(\tau) \mid \sigma < \tau \mbox{ and } \operatorname{dim}(\tau) = d\}$. A very general example is when $K$ is a finite grid complex in $\mathbb{R}^d$, and $f:K \to \mathbb{R}$ records the values of some image defined on the pixels or voxels of $K$.


%a \emph{cubical complex} $K$ arising as the intersection of the integer grid complex $\mathbb{Z}^d$ in $\mathbb{R}^d$ with a rectangle of the form $\Pi_{i=1}^{d}[a_i,b_i]$, where $a_i, b_i \in \mathbb{Z}$. We will call such a complex $K$ a \emph{finite integer grid}. Given a real-valued function $f$ on the $d$-dimensional cells of $K$, this naturally extends to all of $K$ by asserting that $f(\sigma) = \min \{f(\tau) \mid \sigma < \tau \mbox{ and } \operatorname{dim}(\tau) = d\}$. When $d=2$, such a pair $(K,f)$ is simply an image defined over a rectangular array of pixels, and for $d=3$ we have an image defined over a rectangular array of voxels. More generally, we might allow $f$ to take values in $\mathbb{R}^k$ for $k>1$, in which case we have an image with multiple \emph{channels}.\\

%That is, we consider covering $K$ with translates of a fixed subgrid $L$, and denote the dual of the \v{Cech} complex of the cover by $K/L$. If, moreover, we equip $L$ with a function $g: L \to \mathbb{R}$, we have a recipe for producing a function on the top-dimensional cells of $K/L$, which works as follows: Let $U \subset K$ be a set in the cover of $K$ by translates of $L$, which gives rise to a top-dimensional simplex $[U]$ in $K/L$, and define $f \ast g$ on $[U]$ to be $\sum_{\tau \in U} f(\tau)g(\tau)$, where the sum is taken over $d$-dimensional simplices in $U$.  




\end{document}